{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Sardar Jaf\n",
    "#\n",
    "# Generating statistics for a given corpus\n",
    "# corpus structure can be based on directory structure as in:\n",
    "# root\n",
    "#   news\n",
    "#     123.txt\n",
    "#     ...\n",
    "#   economic\n",
    "#      123.txt\n",
    "#      ...\n",
    "#  other directories\n",
    "# or based on file structure as in:\n",
    "#  root\n",
    "#    news.123.txt\n",
    "#    ...\n",
    "#    economics.456.txt\n",
    "#    ....\n",
    "#  OUTOUT\n",
    "#sports:\n",
    "    #Total documents: 1\n",
    "    #Total sentences: 4\n",
    "    #Total words: 170\n",
    "    #Agv. sentence length: 42\n",
    "    #Longest sentence: 24 (in document: sports)\n",
    "    #Shortest sentence: 18 (in document: sports)\n",
    "\n",
    "#economy:\n",
    "    #Total documents: 1\n",
    "    #Total sentences: 3\n",
    "    #Total words: 88\n",
    "    #Agv. sentence length: 29\n",
    "    #Longest sentence: 33 (in document: economy)\n",
    "    #Shortest sentence: 20 (in document: economy)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sardar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "all_words={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9e597e8b44cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#+\"/\"+dirs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mfile_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mfile_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size, chars, firstline)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;31m# we need more data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mnewdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reading files from a directory list and compute various statistics about the corpus\n",
    "# this function works if the corpus structure is directory based, where different files \n",
    "# are placed in different directories and each directory represents a topic \n",
    "\n",
    "\n",
    "# Setting the path for the corpus directory\n",
    "PATH = os.getcwd()\n",
    "dir_path = PATH + \"/corpus_root_directory\" # this should point to the data files\n",
    "dir_list = os.listdir(dir_path)\n",
    "\n",
    "for dirs in dir_list:  \n",
    "    \n",
    "    # store some stats in a dictionary\n",
    "    stats[dirs] = {}\n",
    "    stats[dirs]['total_documents'] = 0\n",
    "    stats[dirs]['total_sentences'] = 0\n",
    "    stats[dirs]['total_words'] = 0\n",
    "    \n",
    "    max_sent_len = 0\n",
    "    min_sent_len = 1000 # there should be no sentence > 1000 words\n",
    "    \n",
    "    all_words[dirs] = [] \n",
    "    for files in os.listdir(dir_path+\"/\"+dirs):\n",
    "        file_input = codecs.open(dir_path+\"/\"+dirs+\"/\"+files, encoding='utf-8')\n",
    "        text = file_input.read()\n",
    "        file_input.close()\n",
    "        \n",
    "        result = text.split('\\n')\n",
    "        for r in result:\n",
    "            if len(r) > 2:\n",
    "                stats[dirs]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "                #if dirs == 'poem':\n",
    "                    #print(r,len(result))\n",
    "                words_tokens = word_tokenize(r)                \n",
    "                all_words[dirs] += words_tokens\n",
    "                stats[dirs]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "                sent_length = len(words_tokens)\n",
    "                \n",
    "                if sent_length >= max_sent_len:\n",
    "                    stats[dirs]['max_sent_len'] = sent_length\n",
    "                    stats[dirs]['max_sent_len_doc'] = dirs+\"/\"+files\n",
    "                \n",
    "                if sent_length <= min_sent_len:\n",
    "                    min_sent_len = sent_length\n",
    "                    stats[dirs]['min_sent_len'] = min_sent_len\n",
    "                    stats[dirs]['min_sent_len_doc'] = dirs+\"/\"+files\n",
    "               \n",
    "        stats[dirs]['total_documents'] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path for the corpus directory\n",
    "PATH = os.getcwd()\n",
    "dir_path = PATH + \"/data\" # this should point to the data files\n",
    "dir_list = os.listdir(dir_path)\n",
    "\n",
    "# reading files from a directory list and compute various statistics about the corpus\n",
    "# this function work on a corpus structure that is based on file names\n",
    "# i.e., the topic name for a file is in the file name e.g., news.123.txt, economy.123.txt\n",
    "for dirs in dir_list:  \n",
    "    \n",
    "    dir_name, file_name = dirs.split('.')[0], dirs.split('.')[1]+'.txt'\n",
    "    # store some stats in a dictionary\n",
    "    stats[dir_name] = {}\n",
    "    stats[dir_name]['total_documents'] = 0\n",
    "    stats[dir_name]['total_sentences'] = 0\n",
    "    stats[dir_name]['total_words'] = 0\n",
    "\n",
    "for dirs in dir_list: \n",
    "    \n",
    "    dir_name, file_name = dirs.split('.')[0], dirs.split('.')[1]+'.txt'\n",
    "    \n",
    "    max_sent_len = 0\n",
    "    min_sent_len = 1000 # there should be no sentence > 1000 words\n",
    "        \n",
    "    all_words[dir_name] = [] \n",
    "    #for files in os.listdir(dir_path+\"/\"+dirs):\n",
    "    file_input = codecs.open(dir_path+\"/\"+dirs, encoding='utf-8')\n",
    "    text = file_input.read()\n",
    "    file_input.close()\n",
    "       \n",
    "    result = text.split('\\n')\n",
    "    for r in result:\n",
    "        if len(r) > 2:\n",
    "            stats[dir_name]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "            #if dirs == 'poem':\n",
    "                #print(r,len(result))\n",
    "            words_tokens = word_tokenize(r)                \n",
    "            all_words[dir_name] += words_tokens\n",
    "            stats[dir_name]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "            sent_length = len(words_tokens)\n",
    "            \n",
    "            if sent_length >= max_sent_len:\n",
    "                stats[dir_name]['max_sent_len'] = sent_length\n",
    "                stats[dir_name]['max_sent_len_doc'] = dir_name\n",
    "               \n",
    "            if sent_length <= min_sent_len:\n",
    "                min_sent_len = sent_length\n",
    "                stats[dir_name]['min_sent_len'] = min_sent_len\n",
    "                stats[dir_name]['min_sent_len_doc'] = dir_name\n",
    "               \n",
    "    stats[dir_name]['total_documents'] += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economy:\n",
      "\tTotal documents: 288\n",
      "\tTotal sentences: 1313\n",
      "\tTotal words: 40640\n",
      "\tAvg. sentence length: 30\n",
      "\tLongest sentence: 35 (in document: economy)\n",
      "\tShortest sentence: 16 (in document: economy)\n",
      "\n",
      "health:\n",
      "\tTotal documents: 115\n",
      "\tTotal sentences: 595\n",
      "\tTotal words: 18887\n",
      "\tAvg. sentence length: 31\n",
      "\tLongest sentence: 24 (in document: health)\n",
      "\tShortest sentence: 17 (in document: health)\n",
      "\n",
      "interviews:\n",
      "\tTotal documents: 634\n",
      "\tTotal sentences: 15433\n",
      "\tTotal words: 564757\n",
      "\tAvg. sentence length: 36\n",
      "\tLongest sentence: 35 (in document: interviews)\n",
      "\tShortest sentence: 35 (in document: interviews)\n",
      "\n",
      "literature:\n",
      "\tTotal documents: 1009\n",
      "\tTotal sentences: 38967\n",
      "\tTotal words: 1115959\n",
      "\tAvg. sentence length: 28\n",
      "\tLongest sentence: 35 (in document: literature)\n",
      "\tShortest sentence: 4 (in document: literature)\n",
      "\n",
      "IT:\n",
      "\tTotal documents: 416\n",
      "\tTotal sentences: 2572\n",
      "\tTotal words: 81018\n",
      "\tAvg. sentence length: 31\n",
      "\tLongest sentence: 71 (in document: IT)\n",
      "\tShortest sentence: 18 (in document: IT)\n",
      "\n",
      "sports:\n",
      "\tTotal documents: 1242\n",
      "\tTotal sentences: 4970\n",
      "\tTotal words: 241612\n",
      "\tAvg. sentence length: 48\n",
      "\tLongest sentence: 50 (in document: sports)\n",
      "\tShortest sentence: 50 (in document: sports)\n",
      "\n",
      "opinion:\n",
      "\tTotal documents: 386\n",
      "\tTotal sentences: 12192\n",
      "\tTotal words: 410811\n",
      "\tAvg. sentence length: 33\n",
      "\tLongest sentence: 16 (in document: opinion)\n",
      "\tShortest sentence: 16 (in document: opinion)\n",
      "\n",
      "theatre:\n",
      "\tTotal documents: 258\n",
      "\tTotal sentences: 10137\n",
      "\tTotal words: 343235\n",
      "\tAvg. sentence length: 33\n",
      "\tLongest sentence: 354 (in document: theatre)\n",
      "\tShortest sentence: 12 (in document: theatre)\n",
      "\n",
      "culture:\n",
      "\tTotal documents: 418\n",
      "\tTotal sentences: 12821\n",
      "\tTotal words: 444948\n",
      "\tAvg. sentence length: 34\n",
      "\tLongest sentence: 31 (in document: culture)\n",
      "\tShortest sentence: 5 (in document: culture)\n",
      "\n",
      "news:\n",
      "\tTotal documents: 2842\n",
      "\tTotal sentences: 29247\n",
      "\tTotal words: 918881\n",
      "\tAvg. sentence length: 31\n",
      "\tLongest sentence: 17 (in document: news)\n",
      "\tShortest sentence: 14 (in document: news)\n",
      "\n",
      "art:\n",
      "\tTotal documents: 439\n",
      "\tTotal sentences: 6766\n",
      "\tTotal words: 334701\n",
      "\tAvg. sentence length: 49\n",
      "\tLongest sentence: 19 (in document: art)\n",
      "\tShortest sentence: 13 (in document: art)\n",
      "\n",
      "poem:\n",
      "\tTotal documents: 51\n",
      "\tTotal sentences: 1160\n",
      "\tTotal words: 11064\n",
      "\tAvg. sentence length: 9\n",
      "\tLongest sentence: 7 (in document: poem)\n",
      "\tShortest sentence: 3 (in document: poem)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_docs = 0\n",
    "total_words = 0\n",
    "total_sents = 0\n",
    "for key, values in stats.items():\n",
    "    #if values['total_sentences']:\n",
    "    #print(key, values['total_documents'], values['total_sentences'], values['total_words'], 'Avg. word/sentence', )\n",
    "    print(\"%s:\" %key)\n",
    "    print(\"\\tTotal documents: %d\" %values['total_documents'])\n",
    "    print(\"\\tTotal sentences: %d\" %values['total_sentences'])\n",
    "    print(\"\\tTotal words: %d\" %values['total_words'])\n",
    "    print(\"\\tAvg. sentence length: %d\" %(values['total_words'] / values['total_sentences']))\n",
    "    print(\"\\tLongest sentence: %d (in document: %s)\"% (values['max_sent_len'], values['max_sent_len_doc']))\n",
    "    print(\"\\tShortest sentence: %d (in document: %s)\\n\"% (values['min_sent_len'], values['min_sent_len_doc']))\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
