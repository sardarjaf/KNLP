{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sardar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "output_dir_path = PATH + \"/plain_text_corpus\"\n",
    "file_dir_list = os.listdir(output_dir_path)\n",
    "\n",
    "input_dir_path = \"/home/sardar/workspace/parse-html/normalised_files\"\n",
    "input_dir_list = os.listdir(input_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "s = ['the cat sat', 'i like milk']\n",
    "s.append('i like her')\n",
    "if not(['the cat sat'] in s):\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "all_words={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the normalised files and segment the sentences\n",
    "# write the segmented sentences in a file\n",
    "for dirs in input_dir_list:\n",
    "        \n",
    "    # store some stats in a dictionary\n",
    "    stats[dirs] = {}\n",
    "    stats[dirs]['total_documents']=0\n",
    "    stats[dirs]['total_sentences']=0\n",
    "    stats[dirs]['total_words']=0\n",
    "    \n",
    "    max_sent_len = 1\n",
    "    min_sent_len = 10000\n",
    "    \n",
    "    all_words[dirs] = []\n",
    "\n",
    "    for files in os.listdir(input_dir_path+\"/\"+dirs)[:1]:\n",
    "        \n",
    "        file_input = codecs.open(input_dir_path+\"/\"+dirs+\"/\"+files, encoding='utf-8')\n",
    "        text = file_input.read()\n",
    "        file_input.close()\n",
    "        \n",
    "        # write the segmented sentences to new files in the directory specified by dir_path variable\n",
    "        file_output = codecs.open(output_dir_path+\"/\"+dirs+\"/\"+files, 'w', encoding='utf-8')\n",
    "        \n",
    "        result = sent_tokenize(text)\n",
    "        for r in result:\n",
    "            if len(r) > 5:\n",
    "                stats[dirs]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "                words_tokens = word_tokenize(r)                \n",
    "                all_words[dirs] += words_tokens\n",
    "                print(words_tokens, len(words_tokens),'\\n')\n",
    "                stats[dirs]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "                sent_length = len(words_tokens)\n",
    "                \n",
    "                if sent_length >= max_sent_len:\n",
    "                   \n",
    "                    stats[dirs]['max_sent_len'] = sent_length\n",
    "                    stats[dirs]['max_sent_len_doc'] = dirs+\"/\"+files\n",
    "                \n",
    "                if sent_length <= min_sent_len:\n",
    "                    min_sent_len = sent_length\n",
    "                    stats[dirs]['min_sent_len'] = min_sent_len\n",
    "                    stats[dirs]['min_sent_len_doc'] = dirs+\"/\"+files\n",
    "               \n",
    "                \n",
    "                file_output.write(r+'\\n')\n",
    "        stats[dirs]['total_documents'] += 1\n",
    "        file_output.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = 0\n",
    "total_words = 0\n",
    "total_sents = 0\n",
    "for key, values in stats.items():\n",
    "    #if values['total_sentences']:\n",
    "    print(key, values['total_documents'], values['total_sentences'], values['total_words'], 'Avg. word/sentence', values['total_words'] / values['total_sentences'])\n",
    "    \n",
    "    print('max_sent_len', values['max_sent_len'])\n",
    "    print('max_sent_len_doc', values['max_sent_len_doc'])\n",
    "    print('min_sent_len', values['min_sent_len'])\n",
    "    print('min_sent_len_doc', values['min_sent_len_doc'])\n",
    "   \n",
    "        \n",
    "\n",
    "\n",
    "print(total_sents, total_words, stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = []\n",
    "for key, value in all_words.items():\n",
    "    print(key,'\\n')\n",
    "    for v in value[:100]:\n",
    "        ws.append(v)\n",
    "print(len(ws))\n",
    "print(set(ws))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = []\n",
    "for dirs in input_dir_list:\n",
    "    print(dirs, len(topics))\n",
    "    topics.append(dirs[0:3])\n",
    "    \n",
    "x = topics\n",
    "plt.ylabel(\"No. Documents\")\n",
    "x = topics\n",
    "y = [1,2,3,4,5,6,7,8,9,22,11,22,33,22,44]\n",
    "y = [1,2,3,4,5,6,7,8,9,22,11,22,33,22,44]\n",
    "N = len(y)\n",
    "x = range(N)\n",
    "width = 1/1.5\n",
    "\n",
    "width = 1/1.5\n",
    "plt.bar(x,y, width, color=\"blue\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
