{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Sardar Jaf\n",
    "#\n",
    "# Generating statistics for a given corpus\n",
    "# corpus structure can be based on directory structure as in:\n",
    "# root\n",
    "#   news\n",
    "#     123.txt\n",
    "#     ...\n",
    "#   economic\n",
    "#      123.txt\n",
    "#      ...\n",
    "#  other directories\n",
    "# or based on file structure as in:\n",
    "#  root\n",
    "#    news.123.txt\n",
    "#    ...\n",
    "#    economics.456.txt\n",
    "#    ....\n",
    "#  OUTOUT\n",
    "#sports:\n",
    "    #Total documents: 1\n",
    "    #Total sentences: 4\n",
    "    #Total words: 170\n",
    "    #Agv. sentence length: 42\n",
    "    #Longest sentence: 24 (in document: sports)\n",
    "    #Shortest sentence: 18 (in document: sports)\n",
    "\n",
    "#economy:\n",
    "    #Total documents: 1\n",
    "    #Total sentences: 3\n",
    "    #Total words: 88\n",
    "    #Agv. sentence length: 29\n",
    "    #Longest sentence: 33 (in document: economy)\n",
    "    #Shortest sentence: 20 (in document: economy)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sardar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "all_words={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news 2285.txt\n"
     ]
    }
   ],
   "source": [
    "# reading files from a directory list and compute various statistics about the corpus\n",
    "# this function works if the corpus structure is directory based, where different files \n",
    "# are placed in different directories and each directory represents a topic \n",
    "\n",
    "\n",
    "# Setting the path for the corpus directory\n",
    "PATH = os.getcwd()\n",
    "dir_path = PATH + \"/corpus_root_directory\" # this should point to the data files\n",
    "dir_list = os.listdir(dir_path)\n",
    "\n",
    "for dirs in dir_list:  \n",
    "    \n",
    "    # store some stats in a dictionary\n",
    "    stats[dirs] = {}\n",
    "    stats[dirs]['total_documents'] = 0\n",
    "    stats[dirs]['total_sentences'] = 0\n",
    "    stats[dirs]['total_words'] = 0\n",
    "    \n",
    "    max_sent_len = 0\n",
    "    min_sent_len = 1000 # there should be no sentence > 1000 words\n",
    "    \n",
    "    all_words[dirs] = [] \n",
    "    for files in os.listdir(dir_path+\"/\"+dirs):\n",
    "        file_input = codecs.open(dir_path+\"/\"+dirs+\"/\"+files, encoding='utf-8')\n",
    "        text = file_input.read()\n",
    "        file_input.close()\n",
    "        \n",
    "        result = text.split('\\n')\n",
    "        for r in result:\n",
    "            if len(r) > 2:\n",
    "                stats[dirs]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "                #if dirs == 'poem':\n",
    "                    #print(r,len(result))\n",
    "                words_tokens = word_tokenize(r)                \n",
    "                all_words[dirs] += words_tokens\n",
    "                stats[dirs]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "                sent_length = len(words_tokens)\n",
    "                \n",
    "                if sent_length >= max_sent_len:\n",
    "                    stats[dirs]['max_sent_len'] = sent_length\n",
    "                    stats[dirs]['max_sent_len_doc'] = dirs+\"/\"+files\n",
    "                \n",
    "                if sent_length <= min_sent_len:\n",
    "                    min_sent_len = sent_length\n",
    "                    stats[dirs]['min_sent_len'] = min_sent_len\n",
    "                    stats[dirs]['min_sent_len_doc'] = dirs+\"/\"+files\n",
    "               \n",
    "        stats[dirs]['total_documents'] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path for the corpus directory\n",
    "PATH = os.getcwd()\n",
    "dir_path = PATH + \"/data\" # this should point to the data files\n",
    "dir_list = os.listdir(dir_path)\n",
    "\n",
    "# reading files from a directory list and compute various statistics about the corpus\n",
    "# this function work on a corpus structure that is based on file names\n",
    "# i.e., the topic name for a file is in the file name e.g., news.123.txt, economy.123.txt\n",
    "for dirs in dir_list:  \n",
    "    \n",
    "    dir_name, file_name = dirs.split('.')[0], dirs.split('.')[1]+'.txt'\n",
    "    # store some stats in a dictionary\n",
    "    stats[dir_name] = {}\n",
    "    stats[dir_name]['total_documents'] = 0\n",
    "    stats[dir_name]['total_sentences'] = 0\n",
    "    stats[dir_name]['total_words'] = 0\n",
    "    \n",
    "    max_sent_len = 0\n",
    "    min_sent_len = 1000 # there should be no sentence > 1000 words\n",
    "        \n",
    "    all_words[dir_name] = [] \n",
    "    #for files in os.listdir(dir_path+\"/\"+dirs):\n",
    "    file_input = codecs.open(dir_path+\"/\"+dirs, encoding='utf-8')\n",
    "    text = file_input.read()\n",
    "    file_input.close()\n",
    "       \n",
    "    result = text.split('\\n')\n",
    "    for r in result:\n",
    "        if len(r) > 2:\n",
    "            stats[dir_name]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "            #if dirs == 'poem':\n",
    "                #print(r,len(result))\n",
    "            words_tokens = word_tokenize(r)                \n",
    "            all_words[dir_name] += words_tokens\n",
    "            stats[dir_name]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "            sent_length = len(words_tokens)\n",
    "            \n",
    "            if sent_length >= max_sent_len:\n",
    "                stats[dir_name]['max_sent_len'] = sent_length\n",
    "                stats[dir_name]['max_sent_len_doc'] = dir_name\n",
    "               \n",
    "            if sent_length <= min_sent_len:\n",
    "                min_sent_len = sent_length\n",
    "                stats[dir_name]['min_sent_len'] = min_sent_len\n",
    "                stats[dir_name]['min_sent_len_doc'] = dir_name\n",
    "               \n",
    "    stats[dir_name]['total_documents'] += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = 0\n",
    "total_words = 0\n",
    "total_sents = 0\n",
    "for key, values in stats.items():\n",
    "    #if values['total_sentences']:\n",
    "    #print(key, values['total_documents'], values['total_sentences'], values['total_words'], 'Avg. word/sentence', )\n",
    "    print(\"%s:\" %key)\n",
    "    print(\"\\tTotal documents: %d\" %values['total_documents'])\n",
    "    print(\"\\tTotal sentences: %d\" %values['total_sentences'])\n",
    "    print(\"\\tTotal words: %d\" %values['total_words'])\n",
    "    print(\"\\tAgv. sentence length: %d\" %(values['total_words'] / values['total_sentences']))\n",
    "    print(\"\\tLongest sentence: %d (in document: %s)\"% (values['max_sent_len'], values['max_sent_len_doc']))\n",
    "    print(\"\\tShortest sentence: %d (in document: %s)\\n\"% (values['min_sent_len'], values['min_sent_len_doc']))\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
