{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Sardar Jaf\n",
    "#\n",
    "# Generating statistics for a given corpus\n",
    "# corpus structure can be based on directory structure as in:\n",
    "# root\n",
    "#   news\n",
    "#     123.txt\n",
    "#     ...\n",
    "#   economic\n",
    "#      123.txt\n",
    "#      ...\n",
    "#  other directories\n",
    "# or based on file structure as in:\n",
    "#  root\n",
    "#    news.123.txt\n",
    "#    ...\n",
    "#    economics.456.txt\n",
    "#    ....\n",
    "#  OUTOUT\n",
    "#sports:\n",
    "    #Total documents: 1\n",
    "    #Total sentences: 4\n",
    "    #Total words: 170\n",
    "    #Agv. sentence length: 42\n",
    "    #Longest sentence: 24 (in document: sports)\n",
    "    #Shortest sentence: 18 (in document: sports)\n",
    "\n",
    "#economy:\n",
    "    #Total documents: 1\n",
    "    #Total sentences: 3\n",
    "    #Total words: 88\n",
    "    #Agv. sentence length: 29\n",
    "    #Longest sentence: 33 (in document: economy)\n",
    "    #Shortest sentence: 20 (in document: economy)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sardar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "all_words={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sardar/ml/KNLP/corpus_root_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-23bce2d97f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdir_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/corpus_root_directory\"\u001b[0m \u001b[0;31m# this should point to the data files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdir_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdirs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sardar/ml/KNLP/corpus_root_directory'"
     ]
    }
   ],
   "source": [
    "# reading files from a directory list and compute various statistics about the corpus\n",
    "# this function works if the corpus structure is directory based, where different files \n",
    "# are placed in different directories and each directory represents a topic \n",
    "\n",
    "\n",
    "# Setting the path for the corpus directory\n",
    "PATH = os.getcwd()\n",
    "dir_path = PATH + \"/corpus_root_directory\" # this should point to the data files\n",
    "dir_list = os.listdir(dir_path)\n",
    "\n",
    "for dirs in dir_list:  \n",
    "    \n",
    "    # store some stats in a dictionary\n",
    "    stats[dirs] = {}\n",
    "    stats[dirs]['total_documents'] = 0\n",
    "    stats[dirs]['total_sentences'] = 0\n",
    "    stats[dirs]['total_words'] = 0\n",
    "    \n",
    "    max_sent_len = 0\n",
    "    min_sent_len = 1000 # there should be no sentence > 1000 words\n",
    "    \n",
    "    all_words[dirs] = [] \n",
    "    for files in os.listdir(dir_path+\"/\"+dirs):\n",
    "        file_input = codecs.open(dir_path+\"/\"+dirs+\"/\"+files, encoding='utf-8')\n",
    "        text = file_input.read()\n",
    "        file_input.close()\n",
    "        \n",
    "        result = text.split('\\n')\n",
    "        for r in result:\n",
    "            if len(r) > 2:\n",
    "                stats[dirs]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "                #if dirs == 'poem':\n",
    "                    #print(r,len(result))\n",
    "                words_tokens = word_tokenize(r)                \n",
    "                all_words[dirs] += words_tokens\n",
    "                stats[dirs]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "                sent_length = len(words_tokens)\n",
    "                \n",
    "                if sent_length >= max_sent_len:\n",
    "                    stats[dirs]['max_sent_len'] = sent_length\n",
    "                    stats[dirs]['max_sent_len_doc'] = dirs+\"/\"+files\n",
    "                \n",
    "                if sent_length <= min_sent_len:\n",
    "                    min_sent_len = sent_length\n",
    "                    stats[dirs]['min_sent_len'] = min_sent_len\n",
    "                    stats[dirs]['min_sent_len_doc'] = dirs+\"/\"+files\n",
    "               \n",
    "        stats[dirs]['total_documents'] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path for the corpus directory\n",
    "PATH = os.getcwd()\n",
    "dir_path = PATH + \"/data\" # this should point to the data files\n",
    "dir_list = os.listdir(dir_path)\n",
    "\n",
    "# reading files from a directory list and compute various statistics about the corpus\n",
    "# this function work on a corpus structure that is based on file names\n",
    "# i.e., the topic name for a file is in the file name e.g., news.123.txt, economy.123.txt\n",
    "for dirs in dir_list:  \n",
    "    \n",
    "    dir_name, file_name = dirs.split('.')[0], dirs.split('.')[1]+'.txt'\n",
    "    # store some stats in a dictionary\n",
    "    stats[dir_name] = {}\n",
    "    stats[dir_name]['total_documents'] = 0\n",
    "    stats[dir_name]['total_sentences'] = 0\n",
    "    stats[dir_name]['total_words'] = 0\n",
    "    unique_words = []\n",
    "\n",
    "for dirs in dir_list: \n",
    "    \n",
    "    dir_name, file_name = dirs.split('.')[0], dirs.split('.')[1]+'.txt'\n",
    "    \n",
    "    max_sent_len = 0\n",
    "    min_sent_len = 1000 # there should be no sentence > 1000 words\n",
    "        \n",
    "    all_words[dir_name] = [] \n",
    "    #for files in os.listdir(dir_path+\"/\"+dirs):\n",
    "    file_input = codecs.open(dir_path+\"/\"+dirs, encoding='utf-8')\n",
    "    text = file_input.read()\n",
    "    file_input.close()\n",
    "       \n",
    "    result = text.split('\\n')\n",
    "    for r in result:\n",
    "        if len(r) > 2:\n",
    "            stats[dir_name]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "            #if dirs == 'poem':\n",
    "                #print(r,len(result))\n",
    "            words_tokens = word_tokenize(r)                \n",
    "            all_words[dir_name] += words_tokens\n",
    "            stats[dir_name]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "            sent_length = len(words_tokens)\n",
    "            \n",
    "            if sent_length >= max_sent_len:\n",
    "                stats[dir_name]['max_sent_len'] = sent_length\n",
    "                stats[dir_name]['max_sent_len_doc'] = dir_name\n",
    "               \n",
    "            if sent_length <= min_sent_len:\n",
    "                min_sent_len = sent_length\n",
    "                stats[dir_name]['min_sent_len'] = min_sent_len\n",
    "                stats[dir_name]['min_sent_len_doc'] = dir_name\n",
    "            \n",
    "            # split line and collect unique words (uw)\n",
    "            for uw in r.split(' '):\n",
    "                if not uw in unique_words:\n",
    "                    unique_words.append(uw)\n",
    "               \n",
    "    stats[dir_name]['total_documents'] += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT:\n",
      "\tTotal documents: 416\n",
      "\tTotal sentences: 2572\n",
      "\tTotal words: 81018\n",
      "\tAvg. sentence length: 31\n",
      "\tLongest sentence: 71 (in document: IT)\n",
      "\tShortest sentence: 18 (in document: IT)\n",
      "\n",
      "art:\n",
      "\tTotal documents: 439\n",
      "\tTotal sentences: 6766\n",
      "\tTotal words: 334701\n",
      "\tAvg. sentence length: 49\n",
      "\tLongest sentence: 19 (in document: art)\n",
      "\tShortest sentence: 13 (in document: art)\n",
      "\n",
      "culture:\n",
      "\tTotal documents: 418\n",
      "\tTotal sentences: 12821\n",
      "\tTotal words: 444948\n",
      "\tAvg. sentence length: 34\n",
      "\tLongest sentence: 31 (in document: culture)\n",
      "\tShortest sentence: 5 (in document: culture)\n",
      "\n",
      "economy:\n",
      "\tTotal documents: 288\n",
      "\tTotal sentences: 1313\n",
      "\tTotal words: 40640\n",
      "\tAvg. sentence length: 30\n",
      "\tLongest sentence: 35 (in document: economy)\n",
      "\tShortest sentence: 16 (in document: economy)\n",
      "\n",
      "health:\n",
      "\tTotal documents: 115\n",
      "\tTotal sentences: 595\n",
      "\tTotal words: 18887\n",
      "\tAvg. sentence length: 31\n",
      "\tLongest sentence: 24 (in document: health)\n",
      "\tShortest sentence: 17 (in document: health)\n",
      "\n",
      "interviews:\n",
      "\tTotal documents: 634\n",
      "\tTotal sentences: 15433\n",
      "\tTotal words: 564757\n",
      "\tAvg. sentence length: 36\n",
      "\tLongest sentence: 35 (in document: interviews)\n",
      "\tShortest sentence: 35 (in document: interviews)\n",
      "\n",
      "literature:\n",
      "\tTotal documents: 1009\n",
      "\tTotal sentences: 38967\n",
      "\tTotal words: 1115959\n",
      "\tAvg. sentence length: 28\n",
      "\tLongest sentence: 35 (in document: literature)\n",
      "\tShortest sentence: 4 (in document: literature)\n",
      "\n",
      "news:\n",
      "\tTotal documents: 2842\n",
      "\tTotal sentences: 29247\n",
      "\tTotal words: 918881\n",
      "\tAvg. sentence length: 31\n",
      "\tLongest sentence: 17 (in document: news)\n",
      "\tShortest sentence: 14 (in document: news)\n",
      "\n",
      "opinion:\n",
      "\tTotal documents: 386\n",
      "\tTotal sentences: 12192\n",
      "\tTotal words: 410811\n",
      "\tAvg. sentence length: 33\n",
      "\tLongest sentence: 16 (in document: opinion)\n",
      "\tShortest sentence: 16 (in document: opinion)\n",
      "\n",
      "poem:\n",
      "\tTotal documents: 51\n",
      "\tTotal sentences: 1160\n",
      "\tTotal words: 11064\n",
      "\tAvg. sentence length: 9\n",
      "\tLongest sentence: 7 (in document: poem)\n",
      "\tShortest sentence: 3 (in document: poem)\n",
      "\n",
      "sports:\n",
      "\tTotal documents: 1242\n",
      "\tTotal sentences: 4970\n",
      "\tTotal words: 241612\n",
      "\tAvg. sentence length: 48\n",
      "\tLongest sentence: 50 (in document: sports)\n",
      "\tShortest sentence: 50 (in document: sports)\n",
      "\n",
      "theatre:\n",
      "\tTotal documents: 258\n",
      "\tTotal sentences: 10137\n",
      "\tTotal words: 343235\n",
      "\tAvg. sentence length: 33\n",
      "\tLongest sentence: 354 (in document: theatre)\n",
      "\tShortest sentence: 12 (in document: theatre)\n",
      "\n",
      "Total number of documents 8098: \n",
      "Total number of sentencess 136173: \n",
      "Total number of words 4526513: \n",
      "Total unique words 581143: \n"
     ]
    }
   ],
   "source": [
    "total_docs = 0\n",
    "total_words = 0\n",
    "total_sents = 0\n",
    "for key, values in sorted(stats.items()):\n",
    "    #if values['total_sentences']:\n",
    "    #print(key, values['total_documents'], values['total_sentences'], values['total_words'], 'Avg. word/sentence', )\n",
    "    print(\"%s:\" %key)\n",
    "    print(\"\\tTotal documents: %d\" %values['total_documents'])\n",
    "    print(\"\\tTotal sentences: %d\" %values['total_sentences'])\n",
    "    print(\"\\tTotal words: %d\" %values['total_words'])\n",
    "    print(\"\\tAvg. sentence length: %d\" %(values['total_words'] / values['total_sentences']))\n",
    "    print(\"\\tLongest sentence: %d (in document: %s)\"% (values['max_sent_len'], values['max_sent_len_doc']))\n",
    "    print(\"\\tShortest sentence: %d (in document: %s)\\n\"% (values['min_sent_len'], values['min_sent_len_doc']))\n",
    "    total_docs += values['total_documents']\n",
    "    total_sents += values['total_sentences']\n",
    "    total_words += values['total_words']\n",
    "    \n",
    "    \n",
    "        \n",
    "print('Total number of documents %d: '%total_docs)\n",
    "print('Total number of sentencess %d: '%total_sents)\n",
    "print('Total number of words %d: '%total_words)\n",
    "print('Total unique words %d: '%len(unique_words))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
