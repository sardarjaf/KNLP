{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the path for the corpus directory\n",
    "PATH = os.getcwd()\n",
    "dir_path = PATH + \"/plain_text_corpus\" # this should point to the data files\n",
    "dir_list = os.listdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "all_words={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reading files from a directory list and compute various statistics about the corpus\n",
    "for dirs in dir_list:  \n",
    "    \n",
    "    # store some stats in a dictionary\n",
    "    stats[dirs] = {}\n",
    "    stats[dirs]['total_documents'] = 0\n",
    "    stats[dirs]['total_sentences'] = 0\n",
    "    stats[dirs]['total_words'] = 0\n",
    "    \n",
    "    max_sent_len = 0\n",
    "    min_sent_len = 1000 # there should be no sentence > 1000 words\n",
    "    \n",
    "    all_words[dirs] = [] \n",
    "    for files in os.listdir(dir_path+\"/\"+dirs):\n",
    "        file_input = codecs.open(dir_path+\"/\"+dirs+\"/\"+files, encoding='utf-8')\n",
    "        text = file_input.read()\n",
    "        file_input.close()\n",
    "        \n",
    "        result = text.split('\\n')\n",
    "        for r in result:\n",
    "            if len(r) > 2:\n",
    "                stats[dirs]['total_sentences'] += 1 # update the total_sentences value in the dictionary for a dir (topic)\n",
    "                #if dirs == 'poem':\n",
    "                    #print(r,len(result))\n",
    "                words_tokens = word_tokenize(r)                \n",
    "                all_words[dirs] += words_tokens\n",
    "                stats[dirs]['total_words'] += (len(words_tokens)) # update total_words value in the dictionary for a dir (topic)\n",
    "                sent_length = len(words_tokens)\n",
    "                \n",
    "                if sent_length >= max_sent_len:\n",
    "                    stats[dirs]['max_sent_len'] = sent_length\n",
    "                    stats[dirs]['max_sent_len_doc'] = dirs+\"/\"+files\n",
    "                \n",
    "                if sent_length <= min_sent_len:\n",
    "                    min_sent_len = sent_length\n",
    "                    stats[dirs]['min_sent_len'] = min_sent_len\n",
    "                    stats[dirs]['min_sent_len_doc'] = dirs+\"/\"+files\n",
    "               \n",
    "        stats[dirs]['total_documents'] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = 0\n",
    "total_words = 0\n",
    "total_sents = 0\n",
    "for key, values in stats.items():\n",
    "    #if values['total_sentences']:\n",
    "    #print(key, values['total_documents'], values['total_sentences'], values['total_words'], 'Avg. word/sentence', )\n",
    "    print(\"%s:\" %key)\n",
    "    print(\"\\tTotal documents: %d\" %values['total_documents'])\n",
    "    print(\"\\tTotal sentences: %d\" %values['total_sentences'])\n",
    "    print(\"\\tTotal words: %d\" %values['total_words'])\n",
    "    print(\"\\tAgv. sentence length: %d\" %(values['total_words'] / values['total_sentences']))\n",
    "    print(\"\\tLongest sentence: %d (in document: %s)\"% (values['max_sent_len'], values['max_sent_len_doc']))\n",
    "    print(\"\\tShortest sentence: %d (in document: %s)\\n\"% (values['min_sent_len'], values['min_sent_len_doc']))\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = []\n",
    "for key, value in all_words.items():\n",
    "    print(key,'\\n')\n",
    "    for v in value[:100]:\n",
    "        ws.append(v)\n",
    "print(len(ws))\n",
    "print(set(ws))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
